\chapter{Elements of Probability}
\label{Appendix_A}
\section{Basic Definitions}
	\begin{definition}
		A probability measure $\mathbb{P}$ on a measurable space $(\Omega, \mathcal{F})$ is a function from $\mathcal{F}$ to $[0, 1]$ such that
			\begin{itemize}
				\item
				$\mathbb{P} (\emptyset) = 0$, and $\mathbb{P} (\Omega) = 1$.
				\item 
				If $\{ A_n \}_{n \geq 1} \in \mathcal{F}$ and $A_i \cap A_j \neq \emptyset$ if $i \neq j$, then $\mathbb{P} (\cup^{\infty}_{n=1} A_n) = \sum^{\infty}_{n = 1} \mathbb{P} (A_n)$.
			\end{itemize}	
	\end{definition}
	
	\noindent A $\sigma$-algebra on a set $X$ is a collection of subsets of $X$ that includes the empty subset and is closed under complement and under countable unions. Denote $\sigma (D) = \cap \{ H : H \text{ is a } \sigma \text{-algebra of } \Omega, D \subseteq H \}$. We call $\sigma (D)$ a $\sigma$-algebra generated by $D$.
	
	\begin{definition}
		A triple  $(\Omega, \mathcal{F}, \mathbb{P})$ is called a probability space if
		\begin{itemize}
			\item
			$\Omega$ is a sample space which is a collection of all samples.
			\item 
			$\mathcal{F}$ is a $\sigma$-algebra on $\Omega$.
			\item
			$\mathbb{P}$ is a probability measure on $(\Omega, \mathcal{F})$. 
		\end{itemize}	
	\end{definition}

	\noindent On a given probability space $(\Omega, \mathcal{F}, \mathbb{P}) (\Omega = \mathbb{R})$, if a cumulative distribution function of a random variable $X$ is normal, i.e.,
	\begin{align}
		\mathbb{P} (X < x) = \displaystyle \int_{-\infty}^{x} \frac{1}{ \sigma \sqrt{2 \pi} } e^{-\frac{(y - \mu)^2}{2 \sigma^2}} dy, \hspace{2mm} \sigma > 0,
	\end{align}
	then the random variable $X$ is called a Gaussian (normal) random variable on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$. Here $X$ is completely characterized by its mean $\mu$ and its standard deviation $\sigma$. We denote $X \sim \mathcal{N} (\mu, \sigma^2)$. The probability density function of $X$ is
	\begin{align*}
		p(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}.
	\end{align*}

	\noindent When $\mu = 0$ and $\sigma = 1$, we call $X$ a standard Gaussian (normal) random variable.
	
	\begin{definition}
		A probability space $(\Omega, \mathcal{F}, \mathbb{P})$ is said to be a complete probability space if for all $B âˆˆ\in \mathcal{F}$ with $\mathbb{P} (B) = 0$ and all $A \subseteq B$ one has $A \in \mathcal{F}$.
	\end{definition}
	
	\begin{definition}
		If $(\Omega, \mathcal{F}, \mathbb{P})$ is a given probability space then a function $Y : \Omega \rightarrow \mathbb{R}^n$ is called $\mathcal{F}$-measurable if $Y^{-1} (U) = \{ w \in \Omega : Y(w) \in U \} \in \mathcal{F}$ holds for all open sets $U \in \mathbb{R}^n$. If $X : \Omega \rightarrow \mathbb{R}^n$ is a function, then $\sigma (X)$ is the smallest $\sigma$-algebra on $\Omega$ containing all the sets $X^{-1} (U)$ for all open sets $U$ in $\mathbb{R}^n$.
	\end{definition}
	
	\begin{definition}
		Suppose that $(\Omega, \mathcal{F}, \mathbb{P})$ is a given complete probability space. A random variable $X$ is an $\mathcal{F}$-measurable function $X : \Omega \rightarrow \mathbb{R}^n$.
	\end{definition}
	
	\noindent Its well known that every random variable induces a probability measure $\mu_X$ (distribution of $X$) on $\mathbb{R}_n$ given by
	\begin{align*}
		\mu_X (B) = \mathbb{P} (X^{-1} (B)).
	\end{align*}
	If $ \int_{\Omega} | X(w) | d \mathbb{P} (w) < \infty$, the expectation of $X$ w.r.t. $\mathbb{P}$ is defined by
	\begin{align*}
		\mathbb{E} [X] = \int_{\Omega} X(w) d \mathbb{P} (w) = \int_{\mathbb{R}^n} x d \mu_X (x).
	\end{align*}  
	Also the $p$-th moment of $X$ is defined as (if the integrals are well defined)
	\begin{align*}
		\mathbb{E} [X^p] = \int_{\Omega} X^p d \mathbb{P} (w) = \int_{\mathbb{R}^n} x^p d \mu_X (x).
	\end{align*}
	The centered moments are defined by $\mathbb{E} \left[ |X - \mathbb{E}[X]| \right]$, $p = 1, 2, \dots, $. When
	$p = 2$, the centered moment is also called the variance. \\
	
	\begin{definition}
		Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space and let $T \subseteq \mathbb{R}$ be
		time. A collection of random variables $X_t$ , $t \in T$ with values in $\mathbb{R}$ is called a
		stochastic process. If time is an interval, $\mathbb{R}^+$ or $\mathbb{R}$, it is called a stochastic process with continuous time. For any fixed $w \in \Omega$, one can regard $X_t (w)$ as a function of
		$t$ (called a sample function of the stochastic process).
	\end{definition}

	\noindent On a probability space $(\Omega, \mathcal{F}, \mathbb{P})$, a filtration refers to an increasing sequence of $\sigma$-algebras:
	\begin{align*}
		\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \mathcal{F}_2 \subseteq \cdots \subseteq \mathcal{F}_n \subseteq \cdots .
	\end{align*}
	
	\noindent A natural filtration (w.r.t. $X$) is the smallest $\sigma$-algebra that contains
	information of $X$. It is generated by $X$ and $\mathcal{F}^X_n = \sigma(X_1, \dots, X_n)$ with $\mathcal{F}^X_0 = \{\emptyset, \Omega \}$. If $\lim_{n \rightarrow \infty} \mathcal{F}_n \subseteq \mathcal{F}$, then we call $(\Omega, \mathcal{F}, \{ \mathcal{F}_n \}_{n \geq 1}, \mathbb{P})$ a filtered probability space. A stochastic process $\{X_n \}$ on a filtered probability space is an adapted process if $X_n$ is $\mathcal{F}_n$-measurable for each $n$.
	
	\begin{definition}
		A family of sub-$\sigma$-algebras $\mathcal{F}_t \subseteq \mathcal{F}$ indexed by $t \in [0, \infty)$ is called a filtration if it is increasing $\mathcal{F}_s \subseteq \mathcal{F}_t$ when $0 \leq s \leq t < \infty$.
	\end{definition}
	
	\begin{definition} 
		A collection of random variables is called a Gaussian process, if the joint distribution of any finite number of its members is Gaussian. In other words, a Gaussian process is a $\mathbb{R}^d$-valued stochastic process with continuous time (or with index) $t$ such that $\left(X(t_0), X(t_1), \dots, X(t_n) \right)^T$ is a $(n+1)$-dimensional Gaussian random vector
		for any $0 \leq t_0 < t_1 < \cdots < t_n$. The Gaussian process is denoted as $X = \{X(t)\}_{t \in I}$ where $I$ is a set of indexes.
	\end{definition}
	
	\begin{definition}
		\label{brownian_motion}
		 A continuous time stochastic process $W(t)$ is called a standard Brownian motion if
		\begin{itemize}
			\item
			$W(t)$ is almost surely continuous in $t$, and $W(0) = 0$. 
			\item
			$W(t)$ has independent increments $W(t_{i+1}) - W(t_i)$ for all $t_n \geq 0$, $i = 0, 1, \dots, n$.
			\item
			$W(t) - W(s) \sim \mathcal{N} (0, t-s)$, i.e., obeys the normal distribution with mean zero and variance $t - s$.
		\end{itemize}	
	\end{definition}

	\noindent Set $x \in D \subset \mathbb{R}^d$, we define infinite dimensional Gaussian processes as follows
	\begin{align*}
		W^Q (x, t) = \displaystyle \sum^{\infty}_{j = 1} \sqrt{q_j} e_j (x) W_j (t),
	\end{align*}
	where $W_j (t)$ are mutually independent Brownian motions. Here $q_j \geq 0$, $j \in \mathbb{N}^d$ and $\{ e_j (x) \}$ is an orthonormal basis in $L^2 (D)$.  The following expansion is usually considered in literature:
	\begin{align*}
		\dot{W}^Q (x, t) = \displaystyle \sum^{\infty}_{j = 1} \sqrt{q_j} e_j (x) \dot{W}_j (t).
	\end{align*}
	where $\dot{W}_j (t) =\frac{d}{dt} W$, is formally the first-order derivative of $W_j (t)$ in time. When $q_j = 1$ for all $j$, we have a space-time white noise, and if $\sum^{\infty}_{j = 1} \sqrt{q_j}$ is called a $Q$-Wiener process. \\

	\noindent The Brownian motion and white noise can also be defined in terms of orthogonal expansions. Suppose that $\{e_j (t)\}_{j \geq 1}$ is a complete orthonormal system in $L^2 ([0, T ])$, then the Brownian motion $W(t)$ can be defined by
	\begin{align}
	    W(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j \int_{0}^{t} e_j (s) ds, \hspace{2mm} t \in [0, T],
	\end{align}
	where $\beta_j$ are mutually independent standard Gaussian random variables for each $j$, and it can be also checked that is indeed a standard Brownian motion. Correspondingly, the white noise is defined by
	\begin{align}
	    \dot{W}(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j e_j (t), \hspace{2mm} t \in [0, T].
    \end{align}

	\begin{definition}
		\label{cylindrical}
		Let $\{e_j\}_{j \geq 1}$ be a complete orthonormal  system of a separable Hilbert space $\mathcal{H}$, and $T \in \mathbb{R}^+$, and $\{\beta_j (t)\}_{j \geq 1}$ be an independent and identically distributed sequence of Brownian Motions. Then a cylindrical Wiener process $W$ in $\mathcal{H}$ is given by
		\begin{align*}
			W(t) = \displaystyle \sum_{j=1}^{\infty} \beta_j e_j (t)
		\end{align*}	
	\end{definition}
	
	\section{Some Important Results in Probability Spaces}
	
	
	It is well known that the Hermite Polynomials $\{P_k (\dot) \}_{k \in \mathbb{N}}$ with $P_0 = 1$ is a complete orthonormal system for $L_2 (\mathbb{R}, \mu_1 (dx))$ with $\mu_1 (dx) = \frac{1}{\sqrt{2 \ pi}} e^{- \frac{x^2}{2}} dx$, which are defined by 
	\begin{align}
		\label{hermite_polynomials}
		P_k(x) = \frac{(-1)^k}{(k!)^{1/2}} e^{\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}}, \hspace{3mm} x \in \mathbb{R}
	\end{align}
	
	
	A functional $\Phi: \mathcal{H} \longrightarrow  \mathbb{R}$, is said to be a smooth simple functional (or a cylinder functional) if there exists a $C^{\infty} $-function $\varphi$ on $\mathbb{R}^n$ and $n$-continuous linear functional $l_1 ,\cdots ,l_n$ on $\mathcal{H}$ such that for $h \in \mathcal{H}$
	\begin{align*}
		\Phi(h) = \phi(h_1, \cdots, h_n) \hspace{2mm} \text{where} \hspace{2mm} h_i = l_i (h), \hspace{2mm} i = 1, \cdots, n.
	\end{align*}
	
	Let $\mathbb{H} = L_2 (\mathcal{H}, \mu)$ denote the Hilbert space of Borel measurable functionals on the probability space with inner product
	\begin{align}
		\label{Inner_Functionals}	
		\langle \Phi, \Psi \rangle_{\mathbb{H}} = \displaystyle \int_{\mathcal{H}} \Phi (v) \Psi (v) \mu (dx), \hspace{2mm} \Phi, \Psi \in \mathbb{H},
	\end{align}
	and the norm $\| \Phi \|_{\mathbb{H}} = \langle \Phi, \Phi\rangle_{\mathbb{H}}^{1/2}$. In $\mathcal{H}$ we choose a basis system $\{\varphi_k \}$ such that $\varphi_k \in \mathcal{H}$.\\	
	
	Let $\Lambda$ an operator such that $Tr < +\infty$. We will denote as $\mathcal{H}_0$ the Hilbert subspace of $\mathcal{H}$ with inner product $\langle g, h \rangle_0 = \langle \Lambda^{1/2} g, \Lambda^{1/2} h \rangle_{\mathcal{H}}$ for $g, h \in \Lambda^{1/2} \mathcal{H}$, which is the completion of $\Lambda^{1/2} \mathcal{H}$ with respect to the norm $\| g \|_0 = \langle g, g \rangle^{1/2}$, and also is dense in $\mathcal{H}$. Then, by using the above can be defined The Hermite functional as follows
	\begin{align}
	\label{hermite_funcionals}
		H_n(h) = \prod_{i=1}^{\infty} P_{n_i} (l_i (h)), h \in \mathcal{H}_0 , n \in J
	\end{align}
	with
	\begin{equation*}
		l_i (h) = \left\langle h, \Lambda^{-1/2} \varphi_i \right\rangle_{\mathcal{H}}, \hspace{3mm} i = 1,2 \cdots
	\end{equation*}
	and the set $\mathcal{J}$ is given by
	\begin{align}
	\label{Conjunto_J}	
		\mathcal{J} = \{\alpha = (\alpha_i,i \geq 1) | \alpha_i \in \mathbb{N}\cup {0}, |\alpha|:= \displaystyle \sum _{i = 0}^{\infty}\alpha_i < \infty\}
	\end{align}
	

	\begin{lemma}
	\label{dense}	
		For $h \in \mathcal{H}$ let $l_i (h) = (h, \Lambda^{-1/2} \varphi_i )_{\mathcal{H}}$, $i=1, 2, \cdots$. Then the set $\{H_n \}$ of all Hermite polynomials on $\mathcal{H}$ forms a complete orthonormal system for $\mathbb{H}$. Hence the set of all functionals are dense in $\mathbb{H}$. Moreover, we have the direct sum decomposition:
		\begin{align*}
		\mathbb{H} = \displaystyle \bigoplus^{\infty}_{j=0} K_j,
		\end{align*}
		where $K_j$ is the subspace of $\mathbb{H}$ spanned by $\{H_n : |n| = j \}$. This result can be found in \cite{DAPRATO1994}.
	\end{lemma}


	\begin{lemma}
		\label{eigen}
		Let $H_n (h)$ be a Hermite polynomial functional given by (\ref{hermite_funcionals}), and suppose that the operator $-A$ have eigenfunctions $e_k$ with eigenvalues $\lambda_k$. Then the operator given in (\ref{operator_L}) satisfies the following
		\begin{align}
			\label{eigen_lambda_kolmo}
			\mathcal{L} H_n (h) = - \lambda_n H_n (h)
		\end{align}
		for any $n \in \mathcal{J}$ and $H_n \in \mathcal{H}$, where
		\begin{align*}
			\lambda_n = \displaystyle \sum_{k=1}^{\infty} n_k \lambda_k
		\end{align*}
	\end{lemma}